{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814f6e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建计算图-LSTM模型\n",
    "#     embendding\n",
    "#     LSTM\n",
    "#     FC\n",
    "#     train_op\n",
    "# 训练流程\n",
    "# 数据集封装\n",
    "#     api: next_batch(batch_size)\n",
    "# 词表封装\n",
    "#     api: sentence2id(text_sentence)\n",
    "# 类别点封装\n",
    "#     api: category2id(text_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749587ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd4d24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_params():\n",
    "    return tf.contrib.training.HParams(\n",
    "        num_embedding_size = 16,\n",
    "        num_timesteps = 50,\n",
    "        num_lstm_nodes = [32, 32],\n",
    "        num_lstm_layers = 2,\n",
    "        num_fc_nodes = 32,\n",
    "        batch_size = 100,\n",
    "        clip_lstm_grads = 1.0,\n",
    "        learning_rate = 0.001,\n",
    "        num_word_threshold = 10,\n",
    "    )\n",
    "\n",
    "hps = get_default_params()\n",
    "\n",
    "train_file = 'cnews_data/cnews.train.seg.txt'\n",
    "val_file = 'cnews_data/cnews.val.seg.txt'\n",
    "test_file = 'cnews_data/cnews.test.seg.txt'\n",
    "vocab_file = 'cnews_data/cnews.vocab.txt'\n",
    "category_file = 'cnews_data/cnews.category.txt'\n",
    "\n",
    "output_folder = 'cnews_data/run_text_rnn'\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.mkdir(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e6a7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, filename, num_word_threshold):\n",
    "        self._word_to_id = {}\n",
    "        self._unk = -1\n",
    "        self._num_word_threshold = num_word_threshold\n",
    "        self._read_dict(filename)\n",
    "    \n",
    "    def _read_dict(self, filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            word, frequency = line.strip('\\r\\n').split('\\t')\n",
    "            frequency = int(frequency)\n",
    "            if frequency < self._num_word_threshold:\n",
    "                continue\n",
    "            idx = len(self._word_to_id)\n",
    "            if word == '<UNK>':\n",
    "                self._unk = idx\n",
    "            self._word_to_id[word] = idx\n",
    "    \n",
    "    def word_to_id(self, word):\n",
    "        return self._word_to_id.get(word, self._unk)\n",
    "    \n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self._word_to_id)\n",
    "    \n",
    "    def sentence_to_id(self, sentence):\n",
    "        word_ids = [self.word_to_id(cur_word) for cur_word in sentence.split()]\n",
    "        return word_ids\n",
    "    \n",
    "class CategoryDict:\n",
    "    def __init__(self, filename):\n",
    "        self._category_to_id = {}\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            category = line.strip('\\r\\n')\n",
    "            idx = len(self._category_to_id)\n",
    "            self._category_to_id[category] = idx\n",
    "        \n",
    "    def category_to_id(self, category_name):\n",
    "        if not category_name in self._category_to_id:\n",
    "            raise Exception(\"%s is not in our category list\" % category_name)\n",
    "        return self._category_to_id[category_name]\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self._category_to_id)\n",
    "\n",
    "vocab = Vocab(vocab_file, hps.num_word_threshold)\n",
    "vocab_size = vocab.size()\n",
    "tf.logging.info('vocab_size: %d', vocab_size)\n",
    "\n",
    "test_str = '的 在 了 是'\n",
    "print(vocab.sentence_to_id(test_str))\n",
    "\n",
    "category_vocab = CategoryDict(category_file)\n",
    "num_classes = category_vocab.size()\n",
    "tf.logging.info(\"%d\", num_classes)\n",
    "test_str = '时尚'\n",
    "tf.logging.info('label: %s, id: %d', test_str, category_vocab.category_to_id(test_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287993e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataSet:\n",
    "    def __init__(self, filename, vocab, category_vocab, num_timesteps):\n",
    "        self._vocab = vocab\n",
    "        self._category_vocab = category_vocab\n",
    "        self._num_timesteps = num_timesteps\n",
    "        self._inputs = [] # matrix\n",
    "        self._outputs = [] # vector\n",
    "        self._indicator = 0\n",
    "        self._parse_file(filename)\n",
    "    \n",
    "    def _parse_file(self, filename):\n",
    "        tf.logging.info('Loading data from %s', filename)\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            label, content = line.strip('\\r\\n').split('\\t')\n",
    "            id_label = self._category_vocab.category_to_id(label)\n",
    "            id_words = self._vocab.sentence_to_id(content)\n",
    "            id_words = id_words[0: self._num_timesteps]\n",
    "            padding_num = self._num_timesteps - len(id_words)\n",
    "            id_words = id_words + [self._vocab.unk for i in range(padding_num)]\n",
    "            self._inputs.append(id_words)\n",
    "            self._outputs.append(id_label)\n",
    "        self._inputs = np.asarray(self._inputs, dtype = np.int32)\n",
    "        self._outputs = np.asarray(self._outputs, dtype = np.int32)\n",
    "        self._random_shuffle()\n",
    "        \n",
    "    def _random_shuffle(self):\n",
    "        p = np.random.permutation(len(self._inputs))\n",
    "        self._inputs = self._inputs[p]\n",
    "        self._outputs = self._outputs[p]\n",
    "        \n",
    "    def next_batch(self, batch_size):\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > len(self._inputs):\n",
    "            self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "            end_indicator = batch_size\n",
    "        if end_indicator > len(self._inputs):\n",
    "            raise Exception(\"batch_size: %d is too large\" % batch_size)\n",
    "        \n",
    "        batch_inputs = self._inputs[self._indicator: end_indicator]\n",
    "        batch_outputs = self._outputs[self._indicator: end_indicator]\n",
    "        self._indicator = end_indicator\n",
    "        return batch_inputs, batch_outputs\n",
    "\n",
    "train_dataset = TextDataSet(train_file, vocab, category_vocab, hps.num_timesteps)\n",
    "val_dataset = TextDataSet(val_file, vocab, category_vocab, hps.num_timesteps)\n",
    "test_dataset = TextDataSet(test_file, vocab, category_vocab, hps.num_timesteps)\n",
    "\n",
    "print(train_dataset.next_batch(2))\n",
    "print(val_dataset.next_batch(2))\n",
    "print(test_dataset.next_batch(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d64542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hps, vocab_size, num_classes):\n",
    "    num_timesteps = hps.num_timesteps\n",
    "    batch_size = hps.batch_size\n",
    "    \n",
    "    inputs = tf.placeholder(tf.int32, (batch_size, num_timesteps))\n",
    "    outputs = tf.placeholder(tf.int32, (batch_size, ))\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    global_step = tf.Variable(tf.zeros([], tf.int64), name='global_step', trainable=False)\n",
    "    \n",
    "    embendding_initializer = tf.random_uniform_initializer(-1.0, 1.0)\n",
    "    with tf.variable_scope('embedding', initializer=embendding_initializer):\n",
    "        embeddings = tf.get_variable('embedding', [vocab_size, hps.num_embedding_size], tf.float32)\n",
    "        embed_inputs = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "        \n",
    "    scale = 1.0 / math.sqrt(hps.num_embedding_size + hps.num_lstm_nodes[-1])\n",
    "    lstm_init = tf.random_uniform_initializer(-scale, scale)\n",
    "    \n",
    "    with tf.variable_scope('lstm_nn', initializer=lstm_init):\n",
    "        cells = []\n",
    "        for i in range(hps.num_lstm_layers):\n",
    "            cell = tf.contrib.rnn.BasicLSTMCell(hps.num_lstm_nodes[i], state_is_tuple=True)\n",
    "            cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "            cells.append(cell)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "        initial_state = cell.zero_state(hps.batch_size, tf.float32)\n",
    "        # [batch_size, num_timesteps, lstm_outputs[-1]]\n",
    "        rnn_outputs, _ = tf.nn.dynamic_rnn(cell, embed_inputs, initial_state=initial_state)\n",
    "        last = rnn_outputs[:, -1, :]\n",
    "    \n",
    "    fc_init = tf.uniform_unit_scaling_initializer(factor=1.0)\n",
    "    with tf.variable_scope('fc', initializer=fc_init):\n",
    "        fc1 = tf.layers.dense(last, hps.num_fc_nodes, activation=tf.nn.relu, name='fc1')\n",
    "        fc1_dropout = tf.contrib.layers.dropout(fc1, keep_prob)\n",
    "        logits = tf.layers.dense(fc1_dropout, num_classes, name='fc2')\n",
    "        \n",
    "    with tf.name_scope('metrics'):\n",
    "        softmax_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=outputs)\n",
    "        loss = tf.reduce_mean(softmax_loss)\n",
    "        y_pred = tf.argmax(tf.nn.softmax(logits), 1, output_type=tf.int32)\n",
    "        correct_pred = tf.equal(outputs, y_pred)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    with tf.name_scope('train_op'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        for var in tvars:\n",
    "            tf.logging.info('variable name: %s', var.name)\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), hps.clip_lstm_grads)\n",
    "        optimizer = tf.train.AdamOptimizer(hps.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(zip(grads, tvars), global_step=global_step)\n",
    "    return ((inputs, outputs, keep_prob), (loss, accuracy), (train_op, global_step))\n",
    "\n",
    "placeholders, metrics, others = create_model(hps, vocab_size, num_classes)\n",
    "\n",
    "inputs, outputs, keep_prob = placeholders\n",
    "loss, accuracy = metrics\n",
    "train_op, global_step = others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be37eb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "train_keep_prob_value = 0.8\n",
    "test_keep_prob_value = 1.0\n",
    "\n",
    "num_train_steps = 10000\n",
    "num_test_steps = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for i in range(num_train_steps):\n",
    "        batch_inputs, batch_labels = train_dataset.next_batch(hps.batch_size)\n",
    "        outputs_val = sess.run([loss, accuracy, train_op, global_step],\n",
    "                          feed_dict = {inputs: batch_inputs, outputs: batch_labels, keep_prob: train_keep_prob_value})\n",
    "        loss_val, accuracy_val, _, global_step_val = outputs_val\n",
    "        if global_step_val % 100 == 0:\n",
    "            tf.logging.info(\"Step: %05d, loss: %3.3f, accuracy: %3.3f\", global_step_val, loss_val, accuracy_val)\n",
    "            \n",
    "        # evalueabtion code\n",
    "        if global_step_val % 1000 == 0:\n",
    "            all_test_acc_val = []\n",
    "            for j in range(num_test_steps):\n",
    "                test_batch_inputs, test_batch_labels = test_dataset.next_batch(hps.batch_size)\n",
    "                test_acc_val = sess.run(\n",
    "                    [accuracy],\n",
    "                    feed_dict = {inputs: test_batch_inputs, outputs: test_batch_labels, keep_prob: test_keep_prob_value}\n",
    "                )[0]\n",
    "                all_test_acc_val.append(test_acc_val)\n",
    "            test_acc_val = np.mean(all_test_acc_val)\n",
    "            print(\"[Test ] Step: %d, acc: %4.5f\" % (global_step_val, test_acc_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a3a5f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
